import os
import torch
import torch.nn as nn
import torch.optim as optim
from tqdm import tqdm
import pandas as pd
from transformers import BertTokenizer, BertModel
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from torch.utils.data import Dataset, DataLoader
import joblib
import json  # âœ… Epoch ì •ë³´ë¥¼ ì €ì¥í•˜ê¸° ìœ„í•œ JSON
import warnings
warnings.filterwarnings("ignore")
from bertClassification import BERTClassifier, WebtoonDataset

# âœ… 3. ì¶”ê°€ í•™ìŠµì´ ê°€ëŠ¥í•˜ë„ë¡ êµ¬í˜„
def continue_training(df, path, epochs_to_train=10):

    if os.path.exists(path):
        df_new = pd.read_excel(path, engine="openpyxl")
        df = pd.concat([df, df_new], ignore_index=True)  # âœ… ê¸°ì¡´ ë°ì´í„°ì™€ í•©ì¹˜ê¸°
        print("âœ… ìƒˆë¡œìš´ ë°ì´í„° ì¶”ê°€ ì™„ë£Œ!")

    # âœ… ì €ì¥ ê²½ë¡œ ì„¤ì •
    local_model_dir = os.path.join(os.path.dirname(__file__), "classifier")
    os.makedirs(local_model_dir, exist_ok=True)

    label_encoder_path = os.path.join(local_model_dir, "label_encoder.pkl")
    model_path = os.path.join(local_model_dir, "bert_webtoon_classifier.pth")
    epoch_info_path = os.path.join(local_model_dir, "epoch_info.json")  # âœ… Epoch ì •ë³´ ì €ì¥ íŒŒì¼

    tokenizer = BertTokenizer.from_pretrained("bert-base-multilingual-cased")

    # âœ… ê¸°ì¡´ ë¼ë²¨ ì¸ì½”ë” ë¡œë“œ ë˜ëŠ” ìƒˆë¡œ ìƒì„±
    if os.path.exists(label_encoder_path):
        label_encoder = joblib.load(label_encoder_path)
        print(f"âœ… ê¸°ì¡´ ë¼ë²¨ ì¸ì½”ë” ë¡œë“œ ì™„ë£Œ: {label_encoder_path}")
    else:
        label_encoder = LabelEncoder()
        print("ğŸš€ ìƒˆë¡œìš´ ë¼ë²¨ ì¸ì½”ë” ìƒì„±")

    df["genre"] = df["genre"].apply(lambda x: eval(x)[0])
    df["labels"] = label_encoder.fit_transform(df["genre"])

    # âœ… ë¼ë²¨ ì¸ì½”ë” ì €ì¥
    joblib.dump(label_encoder, label_encoder_path)
    print(f"âœ… ë¼ë²¨ ì¸ì½”ë” ì €ì¥ ì™„ë£Œ: {label_encoder_path}")

    # âœ… ë°ì´í„° ë¶„í• 
    train_texts, test_texts, train_labels, test_labels = train_test_split(
        df["synopsis"].tolist(), df["labels"].tolist(), test_size=0.2, random_state=42
    )

    train_dataset = WebtoonDataset(train_texts, train_labels, tokenizer)
    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)

    test_dataset = WebtoonDataset(test_texts, test_labels, tokenizer)
    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)  # âœ… í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ë¡œë” ì¶”ê°€

    # âœ… Cuda, MPS, Cpu ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
    device = torch.device("cuda" if torch.cuda.is_available() else "mps"
    if torch.backends.mps.is_available() else "cpu")
    print(f"âœ… ì‹¤í–‰ ì¥ì¹˜: {device}")

    num_classes = len(label_encoder.classes_)

    # âœ… ëª¨ë¸ ë¡œë“œ
    model = BERTClassifier(num_classes=num_classes).to(device)
    start_epoch = 0  # âœ… í•™ìŠµ ì‹œì‘ epoch ê¸°ë³¸ê°’

    # âœ… ê¸°ì¡´ ëª¨ë¸ì´ ìˆë‹¤ë©´ ë¶ˆëŸ¬ì˜¤ê¸°
    if os.path.exists(model_path):
        print(f"âœ… ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì¤‘... ({model_path})")
        model.load_state_dict(torch.load(model_path, map_location=device))
        print("âœ… ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")

        # âœ… ê¸°ì¡´ Epoch ì •ë³´ ë¶ˆëŸ¬ì˜¤ê¸°
        if os.path.exists(epoch_info_path):
            with open(epoch_info_path, "r") as f:
                epoch_info = json.load(f)
                start_epoch = epoch_info.get("last_epoch", 0)  # âœ… ë§ˆì§€ë§‰ í•™ìŠµëœ epoch ë¶ˆëŸ¬ì˜¤ê¸°
                print(f"ğŸ”„ ê¸°ì¡´ í•™ìŠµ Epoch: {start_epoch}")
        else:
            print("âš ï¸ Epoch ì •ë³´ ì—†ìŒ. ì²˜ìŒë¶€í„° í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤.")
    else:
        print("ğŸš€ ìƒˆ ëª¨ë¸ í•™ìŠµ ì‹œì‘!")

    optimizer = optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)
    criterion = nn.CrossEntropyLoss()

    # âœ… ì¶”ê°€ í•™ìŠµ ë£¨í”„
    for epoch in tqdm(range(start_epoch, start_epoch + epochs_to_train)):
        model.train()
        total_loss = 0
        current_lr = optimizer.param_groups[0]['lr']
        print(f"Epoch [{epoch + 1}/{start_epoch + epochs_to_train}], Learning Rate: {current_lr:.6f}")

        for input_ids, attention_mask, labels in train_loader:
            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(input_ids, attention_mask)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        print(f"Epoch [{epoch + 1}/{start_epoch + epochs_to_train}], Loss: {total_loss / len(train_loader):.4f}")

        # âœ… ëª¨ë¸ ì €ì¥
        torch.save(model.state_dict(), model_path)

        # âœ… í•™ìŠµëœ Epoch ì •ë³´ ì €ì¥
        epoch_info = {"last_epoch": epoch + 1}
        with open(epoch_info_path, "w") as f:
            json.dump(epoch_info, f)

        print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ (Epoch: {epoch + 1})")

    print("ğŸ‰ ì¶”ê°€ í•™ìŠµ ì™„ë£Œ!")

    # âœ… í•™ìŠµ í›„ í…ŒìŠ¤íŠ¸ ì •í™•ë„ ì¶œë ¥
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for input_ids, attention_mask, labels in test_loader:
            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)
            outputs = model(input_ids, attention_mask)
            predicted = torch.argmax(outputs, dim=1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    accuracy = 100 * correct / total
    print(f"âœ… í…ŒìŠ¤íŠ¸ ì •í™•ë„: {accuracy:.2f}%")  # âœ… ì •í™•ë„ ì¶œë ¥

# âœ… ì‹¤í–‰
if __name__ == "__main__":
    # âœ… ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
    df = pd.read_excel("../data/NAVER-Webtoon_OSMU.xlsx", engine="openpyxl")

    # âœ… ìƒˆë¡œìš´ ë°ì´í„° ì¶”ê°€
    new_data_path = "../data/new_data.xlsx"
    continue_training(df, new_data_path, epochs_to_train=100)  # ì›í•˜ëŠ” ì¶”ê°€ í•™ìŠµ íšŸìˆ˜ë¥¼ ë³€ê²½ ê°€ëŠ¥